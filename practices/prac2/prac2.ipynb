{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aA14kYCYSwo"
      },
      "source": [
        "## Динамическое программирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roQ4hAstjVce"
      },
      "source": [
        "Рассмотрим алгоритм итерации по оценкам состояния $V$ (Value Iteration):\n",
        "$$\n",
        "V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
        "$$\n",
        "На основе оценки $V_i$ можно посчитать функцию оценки $Q_i$ действия $a$ в состоянии $s$:\n",
        "$$\n",
        "Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]\n",
        "$$\n",
        "$$\n",
        "V_{(i+1)}(s) = \\max_a Q_i(s,a)\n",
        "$$\n",
        "\n",
        "Зададим напрямую модель MDP с картинки:\n",
        "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l6LwgNvgYXIP"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "except ModuleNotFoundError:\n",
        "    COLAB = False\n",
        "    pass\n",
        "\n",
        "if COLAB:\n",
        "    !wget https://raw.githubusercontent.com/Tviskaron/mipt/master/2019/RL/02/mdp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OpKyGJEJYYDn"
      },
      "outputs": [],
      "source": [
        "transition_probs = {\n",
        "  's0':{\n",
        "    'a0': {'s0': 0.5, 's2': 0.5},\n",
        "    'a1': {'s2': 1}\n",
        "  },\n",
        "  's1':{\n",
        "    'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "    'a1': {'s1': 0.95, 's2': 0.05}\n",
        "  },\n",
        "  's2':{\n",
        "    'a0': {'s0': 0.4, 's2': 0.6},\n",
        "    'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
        "  }\n",
        "}\n",
        "rewards = {\n",
        "  's1': {'a0': {'s0': +5}},\n",
        "  's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "\n",
        "from mdp import MDP\n",
        "import numpy as np\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSC6KXuYcsh"
      },
      "source": [
        "Теперь мы можем использовать это MDP, как и любое другое gym окружение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzLyFJ4iYfro"
      },
      "outputs": [],
      "source": [
        "state = mdp.reset()\n",
        "print('initial state =', state)\n",
        "next_state, reward, done, info = mdp.step('a1')\n",
        "print(f'next_state ={next_state}, reward = {reward}, done = {done}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgRdVPJlYjZ4"
      },
      "source": [
        ":Также, помимо стандартных методов, есть дополнительные, которые пригодятся нам для реализации метода итерации по полезностям."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4zK1xXedYn21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_states = ('s0', 's1', 's2')\n",
            "possible_actions('s1') =  ('a0', 'a1')\n",
            "next_states('s1', 'a0') =  {'s0': 0.7, 's1': 0.1, 's2': 0.2}\n",
            "reward('s1', 'a0', 's0') =  5\n",
            "transition_prob('s1', 'a0', 's0') =  0.7\n"
          ]
        }
      ],
      "source": [
        "print(\"all_states =\", mdp.get_all_states())\n",
        "print(\"possible_actions('s1') = \", mdp.get_possible_actions('s1'))\n",
        "print(\"next_states('s1', 'a0') = \", mdp.get_next_states('s1', 'a0'))\n",
        "print(\"reward('s1', 'a0', 's0') = \",mdp.get_reward('s1', 'a0', 's0'))\n",
        "print(\"transition_prob('s1', 'a0', 's0') = \",\n",
        "      mdp.get_transition_prob('s1', 'a0', 's0'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oe_RzZtYq11"
      },
      "source": [
        "### Задание 1\n",
        "\n",
        "Теперь реализуем алгоритм итерации по полезностям, чтобы решить этот вручную заданный MDP. Псевдокод алгоритма:\n",
        "\n",
        "---\n",
        "\n",
        "`1.` Инициализируем $V^{(0)}(s)=0$, для всех $s$\n",
        "\n",
        "`2.` For $i=0, 1, 2, \\dots$\n",
        "\n",
        "`3.` $ \\quad V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$, для всех $s$\n",
        "\n",
        "---\n",
        "\n",
        "Вначале вычисляем оценку состояния-действия:\n",
        "$$Q_i(s, a) = \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aA0DQccjody"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qt0o0MokYv0F"
      },
      "outputs": [],
      "source": [
        "def get_action_value(\n",
        "    mdp, state_values, state, action, gamma\n",
        "):\n",
        "    \"\"\" Вычисляем Q(s,a) по формуле выше \"\"\"\n",
        "    Q = 0\n",
        "    for next_state in mdp.get_next_states(state, action):\n",
        "        p = mdp.get_transition_prob(state, action, next_state)\n",
        "        r = mdp.get_reward(state, action, next_state)\n",
        "        Q += p * (r + gamma * state_values[next_state])\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x06WscSIYysp"
      },
      "outputs": [],
      "source": [
        "test_Vs = {s: i for i, s in enumerate(sorted(mdp.get_all_states()))}\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's2', 'a1', 0.9), 0.69)\n",
        "assert np.isclose(get_action_value(mdp, test_Vs, 's1', 'a0', 0.9), 3.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87q6GhsMY19h"
      },
      "source": [
        "Теперь оцениваем полезность самого состояния, для этого мы можем использовать предыдущий метод:\n",
        "\n",
        "$$V_{(i+1)}(s) = \\max_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = \\max_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O3QFuoVj1iZ"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hFqCuRaBY5J_"
      },
      "outputs": [],
      "source": [
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    if mdp.is_terminal(state):\n",
        "        return 0\n",
        "    \n",
        "    V = max(get_action_value(mdp, state_values, state, action, gamma)\n",
        "            for action in mdp.get_possible_actions(state))\n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lPUyRzQOY8PP"
      },
      "outputs": [],
      "source": [
        "test_Vs_copy = dict(test_Vs)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's0', 0.9), 1.8)\n",
        "assert np.isclose(get_new_state_value(mdp, test_Vs, 's2', 0.9), 1.08)\n",
        "assert np.isclose(get_new_state_value(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9), -13500000000.0), \\\n",
        "   \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
        "assert test_Vs == test_Vs_copy, \"Убедитесь, что вы не изменяете state_values в функции get_new_state_value\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od-SBiPKY_Q3"
      },
      "source": [
        "Теперь создаем основной цикл итерационного оценки полезности состояний с критерием остановки, который проверяет насколько изменились полезности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-0-PlvmkF7P"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uUwb5JCDZDD4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter    0 | diff: 3.50000 | V(start): 0.000 \n",
            "iter    1 | diff: 0.64500 | V(start): 0.000 \n",
            "iter    2 | diff: 0.58050 | V(start): 0.581 \n",
            "iter    3 | diff: 0.43582 | V(start): 0.866 \n",
            "iter    4 | diff: 0.30634 | V(start): 1.145 \n",
            "iter    5 | diff: 0.27571 | V(start): 1.421 \n",
            "iter    6 | diff: 0.24347 | V(start): 1.655 \n",
            "iter    7 | diff: 0.21419 | V(start): 1.868 \n",
            "iter    8 | diff: 0.19277 | V(start): 2.061 \n",
            "iter    9 | diff: 0.17327 | V(start): 2.233 \n",
            "iter   10 | diff: 0.15569 | V(start): 2.389 \n",
            "iter   11 | diff: 0.14012 | V(start): 2.529 \n",
            "iter   12 | diff: 0.12610 | V(start): 2.655 \n",
            "iter   13 | diff: 0.11348 | V(start): 2.769 \n",
            "iter   14 | diff: 0.10213 | V(start): 2.871 \n",
            "iter   15 | diff: 0.09192 | V(start): 2.963 \n",
            "iter   16 | diff: 0.08272 | V(start): 3.045 \n",
            "iter   17 | diff: 0.07445 | V(start): 3.120 \n",
            "iter   18 | diff: 0.06701 | V(start): 3.187 \n",
            "iter   19 | diff: 0.06031 | V(start): 3.247 \n",
            "iter   20 | diff: 0.05428 | V(start): 3.301 \n",
            "iter   21 | diff: 0.04885 | V(start): 3.350 \n",
            "iter   22 | diff: 0.04396 | V(start): 3.394 \n",
            "iter   23 | diff: 0.03957 | V(start): 3.434 \n",
            "iter   24 | diff: 0.03561 | V(start): 3.469 \n",
            "iter   25 | diff: 0.03205 | V(start): 3.502 \n",
            "iter   26 | diff: 0.02884 | V(start): 3.530 \n",
            "iter   27 | diff: 0.02596 | V(start): 3.556 \n",
            "iter   28 | diff: 0.02336 | V(start): 3.580 \n",
            "iter   29 | diff: 0.02103 | V(start): 3.601 \n",
            "iter   30 | diff: 0.01892 | V(start): 3.620 \n",
            "iter   31 | diff: 0.01703 | V(start): 3.637 \n",
            "iter   32 | diff: 0.01533 | V(start): 3.652 \n",
            "iter   33 | diff: 0.01380 | V(start): 3.666 \n",
            "iter   34 | diff: 0.01242 | V(start): 3.678 \n",
            "iter   35 | diff: 0.01117 | V(start): 3.689 \n",
            "iter   36 | diff: 0.01006 | V(start): 3.699 \n",
            "iter   37 | diff: 0.00905 | V(start): 3.708 \n",
            "iter   38 | diff: 0.00815 | V(start): 3.717 \n",
            "iter   39 | diff: 0.00733 | V(start): 3.724 \n",
            "iter   40 | diff: 0.00660 | V(start): 3.731 \n",
            "iter   41 | diff: 0.00594 | V(start): 3.736 \n",
            "iter   42 | diff: 0.00534 | V(start): 3.742 \n",
            "iter   43 | diff: 0.00481 | V(start): 3.747 \n",
            "iter   44 | diff: 0.00433 | V(start): 3.751 \n",
            "iter   45 | diff: 0.00390 | V(start): 3.755 \n",
            "iter   46 | diff: 0.00351 | V(start): 3.758 \n",
            "iter   47 | diff: 0.00316 | V(start): 3.762 \n",
            "iter   48 | diff: 0.00284 | V(start): 3.764 \n",
            "iter   49 | diff: 0.00256 | V(start): 3.767 \n",
            "iter   50 | diff: 0.00230 | V(start): 3.769 \n",
            "iter   51 | diff: 0.00207 | V(start): 3.771 \n",
            "iter   52 | diff: 0.00186 | V(start): 3.773 \n",
            "iter   53 | diff: 0.00168 | V(start): 3.775 \n",
            "iter   54 | diff: 0.00151 | V(start): 3.776 \n",
            "iter   55 | diff: 0.00136 | V(start): 3.778 \n",
            "iter   56 | diff: 0.00122 | V(start): 3.779 \n",
            "iter   57 | diff: 0.00110 | V(start): 3.780 \n",
            "iter   58 | diff: 0.00099 | V(start): 3.781 \n",
            "Принято! Алгоритм сходится!\n"
          ]
        }
      ],
      "source": [
        "def value_iteration(\n",
        "    mdp, state_values=None,\n",
        "    gamma = 0.9, num_iter = 1000, min_difference = 1e-5\n",
        "):\n",
        "    \"\"\" выполняет num_iter шагов итерации по значениям\"\"\"\n",
        "\n",
        "    state_values = state_values or {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        new_state_values = {s: get_new_state_value(mdp, state_values, s, gamma)\n",
        "                            for s in mdp.get_all_states()}\n",
        "\n",
        "        assert isinstance(new_state_values, dict)\n",
        "\n",
        "        # Считаем разницу\n",
        "        diff = max(\n",
        "            abs(new_state_values[s] - state_values[s])\n",
        "            for s in mdp.get_all_states()\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"iter {i:4} | diff: {diff:6.5f} \"\n",
        "            f\"| V(start): {new_state_values[mdp._initial_state]:.3f} \"\n",
        "        )\n",
        "\n",
        "        state_values = new_state_values\n",
        "        if diff < min_difference:\n",
        "            print(\"Принято! Алгоритм сходится!\")\n",
        "            break\n",
        "\n",
        "    return state_values\n",
        "\n",
        "state_values = value_iteration(\n",
        "    mdp, num_iter = 100, min_difference = 0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZKomkPSrZGlZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final state values: {'s0': 3.7810348735476405, 's1': 7.294006423867229, 's2': 4.202140275227048}\n"
          ]
        }
      ],
      "source": [
        "print(\"Final state values:\", state_values)\n",
        "\n",
        "assert abs(state_values['s0'] - 3.781) < 0.01\n",
        "assert abs(state_values['s1'] - 7.294) < 0.01\n",
        "assert abs(state_values['s2'] - 4.202) < 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gz5JxncZJoX"
      },
      "source": [
        "По найденным полезностям и зная модель переходов легко найти оптимальную стратегию:\n",
        "$$\\pi^*(s) = argmax_a \\sum_{s'} P(s' | s,a) \\cdot [ r(s,a,s') + \\gamma V_{i}(s')] = argmax_a Q_i(s,a)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml9AWeYrkNgf"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7gd4m26TZOn3"
      },
      "outputs": [],
      "source": [
        "def get_optimal_action(\n",
        "    mdp, state_values, state, gamma=0.9\n",
        "):\n",
        "    \"\"\" Находим оптимальное действие, используя формулу выше. \"\"\"\n",
        "    if mdp.is_terminal(state): return None\n",
        "\n",
        "    actions = mdp.get_possible_actions(state)\n",
        "    q_values = [get_action_value(mdp, state_values, state, a, gamma) for a in actions]\n",
        "    i = q_values.index(max(q_values))\n",
        "    return actions[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yv3MRqaQZSzs"
      },
      "outputs": [],
      "source": [
        "assert get_optimal_action(mdp, state_values, 's0', 0.9) == 'a1'\n",
        "assert get_optimal_action(mdp, state_values, 's1', 0.9) == 'a0'\n",
        "assert get_optimal_action(mdp, state_values, 's2', 0.9) == 'a1'\n",
        "\n",
        "assert get_optimal_action(mdp, {'s0': -1e10, 's1': 0, 's2': -2e10}, 's0', 0.9) == 'a0', \\\n",
        "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\"\n",
        "assert get_optimal_action(mdp, {'s0': -2e10, 's1': 0, 's2': -1e10}, 's0', 0.9) == 'a1', \\\n",
        "    \"Убедитесь, что вы правильно обрабатываете отрицательные значения Q произвольной величины.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "V1KMZyhbZVVX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average reward:  0.4776\n"
          ]
        }
      ],
      "source": [
        "# Проверим среднее вознаграждение агента\n",
        "\n",
        "s = mdp.reset()\n",
        "rewards = []\n",
        "for _ in range(10000):\n",
        "    s, r, done, _ = mdp.step(get_optimal_action(mdp, state_values, s, 0.9))\n",
        "    rewards.append(r)\n",
        "\n",
        "print(\"average reward: \", np.mean(rewards))\n",
        "\n",
        "assert(0.40 < np.mean(rewards) < 0.55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkokwmulZYYn"
      },
      "source": [
        "### Задание 2\n",
        "\n",
        "Теперь проверим работу итерации по ценностям на классической задаче FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E4V34IMzZbeH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*FFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "iter    0 | diff: 1.00000 | V(start): 0.000 \n",
            "iter    1 | diff: 0.90000 | V(start): 0.000 \n",
            "iter    2 | diff: 0.81000 | V(start): 0.000 \n",
            "iter    3 | diff: 0.72900 | V(start): 0.000 \n",
            "iter    4 | diff: 0.65610 | V(start): 0.000 \n",
            "iter    5 | diff: 0.59049 | V(start): 0.590 \n",
            "iter    6 | diff: 0.00000 | V(start): 0.590 \n",
            "Принято! Алгоритм сходится!\n"
          ]
        }
      ],
      "source": [
        "from mdp import FrozenLakeEnv\n",
        "mdp = FrozenLakeEnv(slip_chance=0)\n",
        "\n",
        "mdp.render()\n",
        "state_values = value_iteration(mdp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNPtPQo2ZdpV"
      },
      "source": [
        "Визуализируем нашу стратегию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aQP4HnjNZg4C"
      },
      "outputs": [],
      "source": [
        "def draw_policy(mdp, state_values, gamma=0.9):\n",
        "    \"\"\"функция визуализации стратегии\"\"\"\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    h, w = mdp.desc.shape\n",
        "    states = sorted(mdp.get_all_states())\n",
        "    V = np.array([state_values[s] for s in states])\n",
        "    Pi = {\n",
        "        s: get_optimal_action(mdp, state_values, s, gamma)\n",
        "        for s in states\n",
        "    }\n",
        "    plt.imshow(\n",
        "        V.reshape(w, h),\n",
        "        cmap='gray', interpolation='none',\n",
        "        clim=(0, 1)\n",
        "    )\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(h) - .5)\n",
        "    ax.set_yticks(np.arange(w) - .5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    Y, X = np.mgrid[0:4, 0:4]\n",
        "    a2uv = {'left': (-1, 0), 'down': (0, -1),\n",
        "            'right': (1, 0), 'up': (-1, 0)}\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            plt.text(x, y, str(mdp.desc[y, x].item()),\n",
        "                     color='g', size=12,\n",
        "                     verticalalignment='center',\n",
        "                     horizontalalignment='center',\n",
        "                     fontweight='bold')\n",
        "            a = Pi[y, x]\n",
        "            if a is None: continue\n",
        "            u, v = a2uv[a]\n",
        "            plt.arrow(x, y, u * .3, -v * .3,\n",
        "                      color='m', head_width=0.1,\n",
        "                      head_length=0.1)\n",
        "    plt.grid(color='b', lw=2, ls='-')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UJ2zkkx2Zlec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after iteration 29\n",
            "iter    0 | diff: 0.00000 | V(start): 0.198 \n",
            "Принято! Алгоритм сходится!\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "mdp = FrozenLakeEnv(map_name='8x8',slip_chance=0.1)\n",
        "state_values = {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(30):\n",
        "    clear_output(True)\n",
        "    print(\"after iteration %i\"%i)\n",
        "    state_values = value_iteration(mdp,\n",
        "                            state_values, num_iter=1)\n",
        "    draw_policy(mdp, state_values)\n",
        "    sleep(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nltqOBDfZoFG"
      },
      "source": [
        "Посмотрим на оптимальную стратегию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CKJ1oJapZq77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*FFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "S*FFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SF*FFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SF*FFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFF*FFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFF*FFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFF*FF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFFF*F\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFF*F\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFF*F\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFH*F\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "right\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHF*\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFF*\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFH*\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFH*\n",
            "FFFHFFFG\n",
            "\n",
            "down\n",
            "\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF*\n",
            "\n"
          ]
        }
      ],
      "source": [
        "s = mdp.reset()\n",
        "mdp.render()\n",
        "for t in range(100):\n",
        "    a = get_optimal_action(mdp, state_values, s, 0.9)\n",
        "    print(a, end='\\n\\n')\n",
        "    s, r, done, _ = mdp.step(a)\n",
        "    mdp.render()\n",
        "    if done:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksq-NonlZtHM"
      },
      "source": [
        "Тестируем на более сложном варианте окружения:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6g4fbKkkZza"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yOBqWNBfZv6v"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iter    0 | diff: 0.80000 | V(start): 0.000 \n",
            "iter    1 | diff: 0.57600 | V(start): 0.000 \n",
            "iter    2 | diff: 0.41472 | V(start): 0.000 \n",
            "iter    3 | diff: 0.29860 | V(start): 0.000 \n",
            "iter    4 | diff: 0.24186 | V(start): 0.000 \n",
            "iter    5 | diff: 0.19349 | V(start): 0.000 \n",
            "iter    6 | diff: 0.15325 | V(start): 0.000 \n",
            "iter    7 | diff: 0.12288 | V(start): 0.000 \n",
            "iter    8 | diff: 0.09930 | V(start): 0.000 \n",
            "iter    9 | diff: 0.08037 | V(start): 0.000 \n",
            "iter   10 | diff: 0.06426 | V(start): 0.000 \n",
            "iter   11 | diff: 0.05129 | V(start): 0.000 \n",
            "iter   12 | diff: 0.04330 | V(start): 0.000 \n",
            "iter   13 | diff: 0.03802 | V(start): 0.033 \n",
            "iter   14 | diff: 0.03332 | V(start): 0.058 \n",
            "iter   15 | diff: 0.02910 | V(start): 0.087 \n",
            "iter   16 | diff: 0.01855 | V(start): 0.106 \n",
            "iter   17 | diff: 0.01403 | V(start): 0.120 \n",
            "iter   18 | diff: 0.00810 | V(start): 0.128 \n",
            "iter   19 | diff: 0.00555 | V(start): 0.133 \n",
            "iter   20 | diff: 0.00321 | V(start): 0.137 \n",
            "iter   21 | diff: 0.00247 | V(start): 0.138 \n",
            "iter   22 | diff: 0.00147 | V(start): 0.139 \n",
            "iter   23 | diff: 0.00104 | V(start): 0.140 \n",
            "iter   24 | diff: 0.00058 | V(start): 0.140 \n",
            "iter   25 | diff: 0.00036 | V(start): 0.141 \n",
            "iter   26 | diff: 0.00024 | V(start): 0.141 \n",
            "iter   27 | diff: 0.00018 | V(start): 0.141 \n",
            "iter   28 | diff: 0.00012 | V(start): 0.141 \n",
            "iter   29 | diff: 0.00007 | V(start): 0.141 \n",
            "iter   30 | diff: 0.00004 | V(start): 0.141 \n",
            "iter   31 | diff: 0.00003 | V(start): 0.141 \n",
            "iter   32 | diff: 0.00001 | V(start): 0.141 \n",
            "iter   33 | diff: 0.00001 | V(start): 0.141 \n",
            "Принято! Алгоритм сходится!\n",
            "Cреднее вознаграждение: 0.763\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0.2, map_name='8x8')\n",
        "state_values = value_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        a = get_optimal_action(mdp, state_values, s, 0.9)\n",
        "        s, r, done, _ = mdp.step(a)\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"Cреднее вознаграждение:\", np.mean(total_rewards))\n",
        "assert(0.6 <= np.mean(total_rewards) <= 0.8)\n",
        "print(\"Принято!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOEo-OheZyYP"
      },
      "source": [
        "### Задание 3\n",
        "\n",
        "Теперь рассмотрим алгоритм итерации по стратегиям (PI, policy iteration):\n",
        "\n",
        "---\n",
        "Initialize $\\pi_0$   `// случайно`\n",
        "\n",
        "For $n=0, 1, 2, \\dots$\n",
        "- Считаем функцию $V^{\\pi_{n}}$\n",
        "- Используя $V^{\\pi_{n}}$, считаем функцию $Q^{\\pi_{n}}$\n",
        "- Получаем новую стратегию: $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
        "---\n",
        "\n",
        "PI включает в себя оценку полезности состояния, как внутренний шаг."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diaeh1f7Z010"
      },
      "source": [
        "Вначале оценим полезности, используя текущую стратегию:\n",
        "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
        "    Мы будем искать точное решение, хотя могли использовать и предыдущий итерационный подход. Для этого будем решать систему линейных уравнений относительно $V^{\\pi}(s_i)$ с помощью np.linalg.solve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pFDjkE2kfsY"
      },
      "source": [
        "### 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-RpV4Yw8Z3bi"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import solve\n",
        "\n",
        "def compute_vpi(mdp, policy, gamma):\n",
        "    \"\"\"\n",
        "    Считем V^pi(s) для всех состояний, согласно стратегии.\n",
        "    :param policy: словарь состояние->действие {s : a}\n",
        "    :returns: словарь {state : V^pi(state)}\n",
        "    \"\"\"\n",
        "    states = mdp.get_all_states()\n",
        "    A, b = [], []\n",
        "    for i, state in enumerate(states):\n",
        "        if state in policy:\n",
        "            a = policy[state]\n",
        "            row = [0] * len(states)\n",
        "            row[i] = 1\n",
        "            reward_sum = 0\n",
        "            for j, next_state in enumerate(states):\n",
        "                p = mdp.get_transition_prob(state, a, next_state)\n",
        "                r = mdp.get_reward(state, a, next_state)\n",
        "                row[j] -= gamma * p\n",
        "                reward_sum += p * r\n",
        "            A.append(row)\n",
        "            b.append(reward_sum)\n",
        "        else:\n",
        "            row = [0] * len(states)\n",
        "            row[i] = 1\n",
        "            A.append(row)\n",
        "            b.append(0)\n",
        "\n",
        "    A = np.array(A)\n",
        "    b = np.array(b)\n",
        "    values = solve(A, b)\n",
        "\n",
        "    state_values = {\n",
        "        states[i] : values[i]\n",
        "        for i in range(len(states))\n",
        "    }\n",
        "    return state_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qeb79E20Z6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'s0': np.float64(0.0), 's1': np.float64(-0.0), 's2': np.float64(0.0)}\n"
          ]
        }
      ],
      "source": [
        "transition_probs = {\n",
        "    's0': {\n",
        "        'a0': {'s0': 0.5, 's2': 0.5},\n",
        "        'a1': {'s2': 1}\n",
        "    },\n",
        "    's1': {\n",
        "        'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
        "        'a1': {'s1': 0.95, 's2': 0.05}\n",
        "    },\n",
        "    's2': {\n",
        "        'a0': {'s0': 0.4, 's1': 0.6},\n",
        "        'a1': {'s0': 0.3, 's1': 0.3, 's2': 0.4}\n",
        "    }\n",
        "}\n",
        "rewards = {\n",
        "    's1': {'a0': {'s0': +5}},\n",
        "    's2': {'a1': {'s0': -1}}\n",
        "}\n",
        "mdp = MDP(transition_probs, rewards, initial_state='s0')\n",
        "\n",
        "gamma = 0.9\n",
        "\n",
        "test_policy = {\n",
        "    s: np.random.choice(mdp.get_possible_actions(s))\n",
        "    for s in mdp.get_all_states()}\n",
        "new_vpi = compute_vpi(mdp, test_policy, gamma)\n",
        "\n",
        "print(new_vpi)\n",
        "assert type(new_vpi) is dict, \\\n",
        "    \"функция compute_vpi должна возвращать словарь \\\n",
        "    {состояние s : V^pi(s) }\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2YNXpxZ9BT"
      },
      "source": [
        "Теперь обновляем стратегию на основе новых значений полезностей:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsYlHPblkrSI"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "TGCHMeaUZ_Qj"
      },
      "outputs": [],
      "source": [
        "def compute_new_policy(mdp, vpi, gamma):\n",
        "    \"\"\"\n",
        "    Рассчитываем новую стратегию\n",
        "    :param vpi: словарь {state : V^pi(state) }\n",
        "    :returns: словарь {state : оптимальное действие}\n",
        "    \"\"\"\n",
        "    Q = {}\n",
        "    for state in mdp.get_all_states():\n",
        "        Q[state] = {}\n",
        "        for a in mdp.get_possible_actions(state):\n",
        "            values = []\n",
        "            for next_state in mdp.get_next_states(state, a):\n",
        "                r = mdp.get_reward(state, a, next_state)\n",
        "                p = mdp.get_transition_prob(\n",
        "                    state, a, next_state\n",
        "                )\n",
        "                values.append(p * (r + gamma * vpi[next_state]))\n",
        "            Q[state][a] = sum(values)\n",
        "\n",
        "    policy = {}\n",
        "    for state in mdp.get_all_states():\n",
        "        actions = mdp.get_possible_actions(state)\n",
        "        if actions:\n",
        "            policy[state] = max(actions, key=lambda a: Q[state][a])\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1b1OXlg9aBsy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'s0': 'a0', 's1': 'a0', 's2': 'a0'}\n"
          ]
        }
      ],
      "source": [
        "new_policy = compute_new_policy(mdp, new_vpi, gamma)\n",
        "\n",
        "print(new_policy)\n",
        "\n",
        "assert type(new_policy) is dict, \\\n",
        "\"функция compute_new_policy должна возвращать словарь \\\n",
        "{состояние s: оптимальное действие}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15mJglOZaEmI"
      },
      "source": [
        "Собираем все в единый цикл:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIrgcIqKkxD2"
      },
      "source": [
        "### 1 балл"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2LcLHHhIaHAZ"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(\n",
        "    mdp, policy=None, gamma = 0.9,\n",
        "    num_iter = 1000, min_difference = 1e-5\n",
        "):\n",
        "    \"\"\"\n",
        "    Запускаем цикл итерации по стратегиям\n",
        "    Если стратегия не определена, задаем случайную\n",
        "    \"\"\"\n",
        "    for i in range(num_iter):\n",
        "        if not policy:\n",
        "            policy = {}\n",
        "            for s in mdp.get_all_states():\n",
        "                if mdp.get_possible_actions(s):\n",
        "                    policy[s] = (\n",
        "                        np.random.choice(mdp.get_possible_actions(s))\n",
        "                    )\n",
        "        state_values = compute_vpi(mdp, policy, gamma)\n",
        "\n",
        "        policy = compute_new_policy(mdp, state_values, gamma)\n",
        "\n",
        "    return state_values, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddfLTSfgaJjU"
      },
      "source": [
        "Тестируем на FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4hLv3X0OaKmg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average reward:  0.886\n",
            "Принято!\n"
          ]
        }
      ],
      "source": [
        "mdp = FrozenLakeEnv(slip_chance=0.1)\n",
        "state_values, policy = policy_iteration(mdp)\n",
        "\n",
        "total_rewards = []\n",
        "for game_i in range(1000):\n",
        "    s = mdp.reset()\n",
        "    rewards = []\n",
        "    for t in range(100):\n",
        "        s, r, done, _ = mdp.step(policy[s])\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            break\n",
        "    total_rewards.append(np.sum(rewards))\n",
        "\n",
        "print(\"average reward: \", np.mean(total_rewards))\n",
        "assert(0.8 <= np.mean(total_rewards) <= 0.95)\n",
        "print(\"Принято!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
